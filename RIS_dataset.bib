@inproceedings{Yang2016,
abstract = {Software GUI testing for laboratory programs is a cumbersome and time-consuming task. In the past studies, a systematic approach for the efficiency issue of testing the laboratory work has not been discussed. In this paper, we propose a learning framework based on the RankBoost learning-to-rank approach to facilitate the verification task by learning to prioritize GUI test cases. In the experiments, we collected Android laboratory programs to investigate the effectiveness of the proposed learning framework. The experimental results show that the proposed learning framework can effectively improve the recall performance based on pairwise priority relations.},
author = {Yang, Cheng Zen and Luo, Yuan Fu and Chien, Yu Jen and Wen, Hsiang Lin},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2952744.2952755},
isbn = {9781450342353},
keywords = {GUI testing,Learning to rank,RankBoost,Test case prioritization},
pages = {12},
publisher = {ACM},
title = {{Learning to prioritize GUI test cases for android laboratory programs}},
volume = {13-15-July},
year = {2016}
}
@inproceedings{Tramontana2020,
abstract = {There is a large need for effective and efficient testing processes and tools for mobile applications, due to their continuous evolution and to the sensitivity of their users to failures. Industries and researchers focus their effort to the realization of effective fully automatic testing techniques for mobile applications. Many of the proposed testing techniques lack in efficiency because their algorithms cannot be executed in parallel. In particular, Active Learning testing techniques usually relay on sequential algorithms. In this paper we propose a Active Learning technique for the fully automatic exploration and testing of Android applications, that parallelizes and improves a general algorithm proposed in the literature. The novel parallel algorithm has been implemented in the context of a prototype tool exploiting a component-based architecture, and has been experimentally evaluated on 3 open source Android applications by varying different deployment configurations. The measured results have shown the feasibility of the proposed technique and an average saving in testing time between 33% (deploying two testing resources) and about 80% (deploying 12 testing resources).},
author = {Tramontana, Porfirio and Amatucci, Nicola and Fasolino, Anna Rita},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-64881-7_11},
isbn = {9783030648800},
issn = {16113349},
pages = {169--185},
publisher = {Springer},
title = {{A Technique for Parallel GUI Testing of Android Applications}},
volume = {12543 LNCS},
year = {2020}
}
@inproceedings{Adamo2018,
abstract = {This paper presents a reinforcement learning approach to automated GUI testing of Android apps. We use a test generation algorithm based on Q-learning to systematically select events and explore the GUI of an application under test without requiring a preexisting abstract model. We empirically evaluate the algorithm on eight Android applications and find that the proposed approach generates test suites that achieve between 3.31% to 18.83% better block-level code coverage than random test generation.},
author = {Adamo, David and Khan, Md Khorrom and Koppula, Sreedevi and Bryce, Ren{\'{e}}e},
booktitle = {A-TEST 2018 - Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation, Co-located with FSE 2018},
doi = {10.1145/3278186.3278187},
isbn = {9781450360531},
keywords = {Android,GUI testing,Mobile app,Q-learning},
pages = {2--8},
publisher = {ACM},
title = {{Reinforcement learning for android GUI testing}},
year = {2018}
}
@inproceedings{Azim2013a,
abstract = {Systematic exploration of Android apps is an enabler for a variety of app analysis and testing tasks. Performing the exploration while apps run on actual phones is essential for exploring the full range of app capabilities. However, exploring real-world apps on real phones is challenging due to non-determinism, non-standard control flow, scalability and overhead constraints. Relying on end-users to conduct the exploration might not be very effective: we performed a 7-user study on popular Android apps, and found that the combined 7-user coverage was 30.08% of the app screens and 6.46% of the app methods. Prior approaches for automated exploration of Android apps have run apps in an emulator or focused on small apps whose source code was available. To address these problems, we present A3E, an approach and tool that allows substantial Android apps to be explored systematically while running on actual phones, yet without requiring access to the app's source code. The key insight of our approach is to use a static, taint-style, dataflow analysis on the app bytecode in a novel way, to construct a high-level control flow graph that captures legal transitions among activities (app screens). We then use this graph to develop an exploration strategy named Targeted Exploration that permits fast, direct exploration of activities, including activities that would be difficult to reach during normal use. We also developed a strategy named Depth-first Exploration that mimics user actions for exploring activities and their constituents in a slower, but more systematic way. To measure the effectiveness of our techniques, we use two metrics: activity coverage (number of screens explored) and method coverage. Experiments with using our approach on 25 popular Android apps including BBC News, Gas Buddy, Amazon Mobile, YouTube, Shazam Encore, and CNN, show that our exploration techniques achieve 59.39-64.11% activity coverage and 29.53-36.46% method coverage. Copyright {\textcopyright} 2013. Copyright {\textcopyright} 2013 ACM.},
author = {Azim, Tanzirul and Neamtiu, Iulian},
booktitle = {ACM SIGPLAN Notices},
doi = {10.1145/2544173.2509549},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Azim - Unknown - Targeted and Depth-ﬁrst Exploration.pdf.pdf:pdf},
isbn = {145032374X},
issn = {15232867},
keywords = {Code coverage,Dynamic analysis,GUI testing,Google Android,Greybox testing,Systematic exploration,Taint analysis,Test case generation},
number = {10},
pages = {641--660},
publisher = {ACM},
title = {{Targeted and depth-first exploration for systematic testing of Android apps}},
volume = {48},
year = {2013}
}
@inproceedings{Yeh2013,
abstract = {In Android system, black box testing has risen to three key issues: S1: There is no source code and for tester to know the internal logic of the testing App. S2: There is no testing criterion for tester to know the correct behavior and testing scope of the testing App. S3: It is difficult to measure the testing coverage without instrumentation the testing App. In this paper, we provide an approach to analyze the GUI model during the testing process, implement the black-box based android GUI testing system and select 7 Apps for evaluation. Finally we compare our result of our system with the monkey tool [1] and discuss the inner App's properties that influence on the testing result.},
author = {Yeh, Chao Chun and Huang, Shih Kun and Chang, Sung Yen},
booktitle = {MobiSys 2013 - Proceedings of the 11th Annual International Conference on Mobile Systems, Applications, and Services},
doi = {10.1145/2462456.2465717},
isbn = {9781450316729},
keywords = {Market App software,Software quality,Software testing},
pages = {529--530},
publisher = {ACM},
title = {{Poster: A black-box based android GUI testing system}},
year = {2013}
}
@inproceedings{Pan2020,
abstract = {Mobile applications play an important role in our daily life, while it still remains a challenge to guarantee their correctness. Model-based and systematic approaches have been applied to Android GUI testing. However, they do not show significant advantages over random approaches because of limitations such as imprecise models and poor scalability. In this paper, we propose Q-testing, a reinforcement learning based approach which benefits from both random and model-based approaches to automated testing of Android applications. Q-testing explores the Android apps with a curiosity-driven strategy that utilizes a memory set to record part of previously visited states and guides the testing towards unfamiliar functionalities. A state comparison module, which is a neural network trained by plenty of collected samples, is novelly employed to divide different states at the granularity of functional scenarios. It can determine the reinforcement learning reward in Q-testing and help the curiosity-driven strategy explore different functionalities efficiently. We conduct experiments on 50 open-source applications where Q-testing outperforms the state-of-the-art and state-of-practice Android GUI testing tools in terms of code coverage and fault detection. So far, 22 of our reported faults have been confirmed, among which 7 have been fixed.},
author = {Pan, Minxue and Huang, An and Wang, Guoxin and Zhang, Tian and Li, Xuandong},
booktitle = {ISSTA 2020 - Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
doi = {10.1145/3395363.3397354},
isbn = {9781450380089},
keywords = {Android,functional scenario division,reinforcement learning},
pages = {153--164},
title = {{Reinforcement learning based curiosity-driven testing of Android applications}},
year = {2020}
}
@inproceedings{Li2019,
abstract = {Automated input generators must constantly choose which UI element to interact with and how to interact with it, in order to achieve high coverage with a limited time budget. Currently, most black-box input generators adopt pseudo-random or brute-force searching strategies, which may take very long to find the correct combination of inputs that can drive the app into new and important states. We propose Humanoid, an automated black-box Android app testing tool based on deep learning. The key technique behind Humanoid is a deep neural network model that can learn how human users choose actions based on an app's GUI from human interaction traces. The learned model can then be used to guide test input generation to achieve higher coverage. Experiments on both open-source apps and market apps demonstrate that Humanoid is able to reach higher coverage, and faster as well, than the state-of-the-art test input generators. Humanoid is open-sourced at https://github.com/yzygitzh/Humanoid and a demo video can be found at https://youtu.be/PDRxDrkyORs.},
archivePrefix = {arXiv},
arxivId = {1901.02633},
author = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
booktitle = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
doi = {10.1109/ASE.2019.00104},
eprint = {1901.02633},
isbn = {9781728125084},
keywords = {Android,Automated test input generation,Deep learning,Graphical user interface,Mobile app,Software testing},
pages = {1070--1073},
publisher = {IEEE},
title = {{Humanoid: A deep learning-based approach to automated black-box android app testing}},
year = {2019}
}
@inproceedings{Haoyin2017,
abstract = {Android is one of the most popular operating system for smart phone and tablets. The fast growing of applications in the market with complex functionalities makes the testing of them a demanding task. Random testing is an effective measure to help testers to expose the bugs within the application. However, the built-in Monkey tool is still too primitive to expose bugs effectively. In this paper, we propose our improved monkeytesting tool, which improves the fault-detection capability of the monkey tool significantly with a series of optimizations. The empirical study shows that our improved monkey tool is more effective than the monkey on real-life Android applications.},
author = {Haoyin, L. V.},
booktitle = {Proceedings of the 2017 International Conference on Wireless Communications, Signal Processing and Networking, WiSPNET 2017},
doi = {10.1109/WiSPNET.8299722},
isbn = {9781509044412},
keywords = {Android,Random testing,Random walk,Test case generation},
pages = {72--76},
publisher = {IEEE},
title = {{Automatic android application GUI testing-A random walk approach}},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{Subramanian2016,
abstract = {Mobile devices such as smartphones and tablets have become an integral part of a person's life. These portable devices opened up a new software market for mobile application development resulting in various applications from healthcare, banking till entertainment. Therefore, there is a need for mobile applications to be reliable and maintainable. In this paper we introduce an equivalent class based technique for testing the graphical user interface of Android applications. This technique is a specification based approach, in which test cases are generated based on the functionalities and the graphical user interface specification. For each possible user interface event a set of test cases are generated using equivalence class partitioning approach. Once the test cases are generated for the given application, the app is executed based on the generated test cases and results are compared with the other testing techniques. From the obtained results we can infer that our approach detects more bugs than other previous work. In addition, this approach helps in the generation of test cases at an early in the app development life cycle.},
author = {Subramanian, Sathyanarayanan and Singleton, Thomas and Ariss, Omar El},
booktitle = {2016 International Conference on System Reliability and Science, ICSRS 2016 - Proceedings},
doi = {10.1109/ICSRS.2016.7815843},
isbn = {9781509032778},
keywords = {Android,Equivalence class partitioning,GUI testing,Specification based approach},
pages = {84--89},
publisher = {IEEE},
title = {{Class coverage GUI testing for Android applications}},
year = {2017}
}
@article{Lin2014,
abstract = {Automated GUI testing consists of simulating user events and validating the changes in the GUI in order to determine if an Android application meets specifications. Traditional record-replay testing tools mainly focus on facilitating the test case writing process but not the replay and verification process. The accuracy of testing tools degrades significantly when the device under test (DUT) is under heavy load. In order to improve the accuracy, our previous work, SPAG, uses event batching and smart wait function to eliminate the uncertainty of the replay process and adopts GUI layout information to verify the testing results. SPAG maintains an accuracy of up to 99.5 percent and outperforms existing methods. In this work, we propose smart phone automated GUI testing tool with camera (SPAG-C), an extension of SPAG, to test an Android hardware device. Our goal is to further reduce the time required to record test cases and increase reusability of the test oracle without compromising test accuracy. In the record stage, SPAG captures screenshots from device's frame buffer and writes verification commands into the test case. Unlike SPAG, SPAG-C captures the screenshots from an external camera instead of frame buffer. In the replay stage, SPAG-C automatically performs image comparison while SPAG simply performs a string comparison to verify the test results. In order to make SPAG-C reusable for different devices and to allow bettersynchronization at the time of capturing images, we develop a new architecture that uses an external camera and Web services to decouple the test oracle. Our experiments show that recording a test case using SPAG-C's automatic verification is as fast as SPAG's but more accurate. Moreover, SPAG-C is 50 to 75 percent faster than SPAG in achieving the same test accuracy. With reusability, SPAG-C reduces the testing time from days to hours for heterogeneous devices.},
author = {Lin, Ying Dar and Rojas, Jose F. and Chu, Edward T.H. and Lai, Yuan Cheng},
doi = {10.1109/TSE.2014.2331982},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Reusable software,test execution,testing tools,user interfaces},
number = {10},
pages = {957--970},
publisher = {IEEE},
title = {{On the accuracy, efficiency, and reusability of automated test oracles for android devices}},
volume = {40},
year = {2014}
}
@inproceedings{Takala2011,
abstract = {This paper presents experiences in model-based graphical user interface testing of Android applications. We present how model-based testing and test automation was implemented with Android, including how applications were modeled, how tests were designed and executed, and what kind of problems were found in the tested application during the whole process. The main focus is on a case study that was performed with an Android application, the BBC News Widget. Our goal is to present actual data on the experiences and to discuss if advantages can be gained using model-based testing when compared with traditional graphical user interface testing. Another contribution of this paper is a description of a keyword-based test automation tool that was implemented for the Android emulator during the case study. All the models and the tools created or used in this case study are available as open source. {\textcopyright} 2011 IEEE.},
author = {Takala, Tommi and Katara, Mika and Harty, Julian},
booktitle = {Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation, ICST 2011},
doi = {10.1109/ICST.2011.11},
isbn = {9780769543420},
keywords = {Android,Automatic GUI Testing,Automatic Test Generation,Driven Testing,Model-Based Testing},
pages = {377--386},
publisher = {IEEE},
title = {{Experiences of system-level model-based GUI testing of an android application}},
year = {2011}
}
@article{Song2017b,
abstract = {With the prevalence of Android-based mobile devices, automated testing for Android apps has received increasing attention. However, owing to the large variety of events that Android supports, test input generation is a challenging task. In this paper, we present a novel approach and an open source tool called EHBDroid for testing Android apps. In contrast to conventional GUI testing approaches, a key novelty of EHBDroid is that it does not generate events from the GUI, but directly invokes callbacks of event handlers. By doing so, EHBDroid can efficiently simulate a large number of events that are difficult to generate by traditional UI-based approaches. We have evaluated EHBDroid on a collection of 35 real-world large-scale Android apps and compared its performance with two state-of-the-art UI-based approaches, Monkey and Dynodroid. Our experimental results show that EHBDroid is significantly more effective and efficient than Monkey and Dynodroid: in a much shorter time, EHBDroid achieves as much as 22.3% higher statement coverage (11.1% on average) than the other two approaches, and found 12 bugs in these benchmarks, including 5 new bugs that the other two failed to find.},
author = {Song, Wei and Qian, Xiangxing and Huang, Jeff},
doi = {10.1109/ASE.2017.8115615},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Qian, Huang - 2017 - EHBDroid Beyond GUI testing for Android applications.pdf:pdf},
isbn = {9781538626849},
journal = {ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
keywords = {Android,automated testing,event generation,event handlers},
pages = {27--37},
title = {{EHBDroid: Beyond GUI testing for Android applications}},
year = {2017}
}
@inproceedings{Lin2018,
abstract = {Android GUI testing is an important research field to maintain software quality of Android apps. Although many GUI testing schemes have been investigated in the past, the discussion of exploration event scheduling has not been investigated comprehensively for the event replay process. Based on the component priority relationship, this study proposes a novel exploration event scheduling scheme called CPR to reduce the number of the testing events in the replay process. Compared with the breadth-first traversal scheme, the proposed CPR scheme can reduce up to 62% of testing events for achieving the same component coverage, and up to 69% of testing events for layout traversal. Compared with the depth-first traversal scheme, CPR can reduce up to 15% of testing events for achieving the same component coverage, and up to 42% of testing events for layout traversal. With respect to the testing time, CPR can achieve the best performance for most of the AUTs in the empirical study. The results of the empirical experiments show that the proposed CPR scheduling scheme can have the benefits in improving the testing performance.},
author = {Lin, Chia Hui and Yang, Cheng Zen and Lu, Peng and Lin, Tzu Heng and You, Zhi Jun},
booktitle = {Proceedings - International Computer Software and Applications Conference},
doi = {10.1109/COMPSAC.2018.00020},
isbn = {9781538626665},
issn = {07303157},
keywords = {Event Scheduling,GUI Testing,Priority Tree,Replay Events},
pages = {90--99},
publisher = {IEEE},
title = {{Exploration Scheduling for Replay Events in GUI Testing on Android Apps}},
volume = {1},
year = {2018}
}
@inproceedings{Machiry2013,
abstract = {We present a system Dynodroid for generating relevant inputs to unmodified Android apps. Dynodroid views an app as an event-driven program that interacts with its environment by means of a sequence of events through the Android framework. By instrumenting the framework once and for all, Dynodroid monitors the reaction of an app upon each event in a lightweight manner, using it to guide the generation of the next event to the app. Dynodroid also allows interleaving events from machines, which are better at generating a large number of simple inputs, with events from humans, who are better at providing intelligent inputs. We evaluated Dynodroid on 50 open-source Android apps, and compared it with two prevalent approaches: users manually exercising apps, and Monkey, a popular fuzzing tool. Dynodroid, humans, and Monkey covered 55%, 60%, and 53%, respectively, of each app's Java source code on average. Monkey took 20X more events on average than Dynodroid. Dynodroid also found 9 bugs in 7 of the 50 apps, and 6 bugs in 5 of the top 1, 000 free apps on Google Play. Copyright 2013 ACM.},
author = {Machiry, Aravind and Tahiliani, Rohan and Naik, Mayur},
booktitle = {2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings},
doi = {10.1145/2491411.2491450},
isbn = {9781450322379},
keywords = {Android,GUI testing,Testing event-driven programs},
pages = {224--234},
publisher = {ACM},
title = {{Dynodroid: An input generation system for android apps}},
year = {2013}
}
@inproceedings{Dong2020,
abstract = {Android testing tools generate sequences of input events to exercise the state space of the app-under-test. Existing search-based techniques systematically evolve a population of event sequences so as to achieve certain objectives such as maximal code coverage. The hope is that the mutation of fit event sequences leads to the generation of even fitter sequences. However, the evolution of event sequences may be ineffective. Our key insight is that pertinent app states which contributed to the original sequence's fitness may not be reached by a mutated event sequence. The original path through the state space is truncated at the point of mutation. In this paper, we propose instead to evolve a population of states which can be captured upon discovery and resumed when needed. The hope is that generating events on a fit program state leads to the transition to even fitter states. For instance, we can quickly deprioritize testing the main screen state which is visited by most event sequences, and instead focus our limited resources on testing more interesting states that are otherwise difficult to reach. We call our approach time-travel testing because of this ability to travel back to any state that has been observed in the past. We implemented time-travel testing into TimeMachine, a time-travel enabled version of the successful, automated Android testing tool Monkey. In our experiments on a large number of open- and closed source Android apps, TimeMachine outperforms the state-of-theart search-based/model-based Android testing tools Sapienz and Stoat, both in terms of coverage achieved and crashes found.},
author = {Dong, Zhen and Bohme, Marcel and Cojocaru, Lucia and Roychoudhury, Abhik},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/3377811.3380402},
isbn = {9781450371216},
issn = {02705257},
pages = {481--492},
publisher = {IEEE},
title = {{Time-travel testing of android apps}},
year = {2020}
}
@article{Ngo2020,
author = {Ngo, Chanh Duc and Pastore, Fabrizio and Briand, Lionel},
journal = {arXiv preprint arXiv:2012.02471},
title = {{Automated, Cost-effective, and Update-driven App Testing}},
year = {2020}
}
@inproceedings{Zeng2016,
abstract = {Monkey, a random testing tool from Google, has been popularly used in industrial practices for automatic test input generation for Android due to its applicability to a variety of application settings, e.g., ease of use and compatibility with different Android platforms. Recently, Monkey has been under the spotlight of the research community: Recent studies found out that none of the studied tools from the academia were actually better than Monkey when applied on a set of open source Android apps. Our recent efforts performed the first case study of applying Monkey on WeChat, a popular messenger app with over 800 million monthly active users, and revealed many limitations of Monkey along with developing our improved approach to alleviate some of these limitations. In this paper, we explore two optimization techniques to improve the effectiveness and efficiency of our previous approach. We also conduct manual categorization of not-covered activities and two automatic coverage-analysis techniques to provide insightful information about the not-covered code entities. Lastly, we present findings of our empirical studies of conducting automatic random testing on WeChat with the preceding techniques.},
author = {Zheng, Haibing and Li, Dengfeng and Liang, Beihai and Zeng, Xia and Zheng, Wujie and Deng, Yuetang and Lam, Wing and Yang, Wei and Xie, Tao},
booktitle = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017},
doi = {10.1109/ICSE-SEIP.2017.32},
isbn = {9781538627174},
pages = {253--262},
publisher = {ACM},
title = {{Automated test input generation for android: Towards getting there in an industrial case}},
year = {2017}
}
@inproceedings{Kaasila2012,
abstract = {Open mobile platforms such as Android currently suffer from the existence of multiple versions, each with its own peculiarities. This makes the comprehensive testing of interactive applications challenging. In this paper we present Testdroid, an online platform for conducting scripted user interface tests on a variety of physical Android handsets. Testdroid allows developers and researchers to record test scripts, which along with their application are automatically executed on a variety of handsets in parallel. The platform reports the outcome of these tests, enabling developers and researchers to quickly identify platforms where their systems may crash or fail. At the same time the platform allows us to identify more broadly the various problems associated with each handset, as well as frequent programming mistakes. Copyright 2012 ACM.},
author = {Kaasila, Jouko and Ferreira, Denzil and Kostakos, Vassilis and Ojala, Timo},
booktitle = {Proceedings of the 11th International Conference on Mobile and Ubiquitous Multimedia, MUM 2012},
doi = {10.1145/2406367.2406402},
isbn = {9781450318150},
keywords = {Applications,Fragmentation,Mobile app,Performance,Remote testing,Usability,User interface},
pages = {28},
publisher = {ACM},
title = {{Testdroid: Automated remote UI testing on android}},
year = {2012}
}
@inproceedings{Ji2018,
abstract = {At present, Android automated GUI testing has been widely used in mobile application testing. Automated GUI test input generation technology and tools are hot topics for practitioners, but errors in some test screenshots generated by automated test input tools still need to be reviewed manually. In this paper, we creatively proposed an automatic detection platform for GUI errors, detecting the GUI errors of mobile related and image-related widget error classification model through machine learning, which detects the error of widgets. On all experimental App test sets, the accuracy of the text-related widget error classification model reached an average of 98.06%, and the accuracy of image-related widgets error classification model achieved an average of 95.44%, which greatly reduced the time cost of reviewing GUI errors manually. In addition, we analyze the relative positional relationship between the widgets, and use the Wilson score sorting algorithm to analyze the symbiosis and interdependence between the widgets, and finally generate the assertion tables, thus more complex GUI errors can be detected.},
author = {Ji, Meichen},
booktitle = {Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
doi = {10.1109/ICSESS.2018.8663923},
isbn = {9781538665640},
issn = {23270594},
keywords = {GUI error detection,assertions,automated GUI testing,automated traversal tool,machine learning},
pages = {957--961},
publisher = {IEEE},
title = {{UIChecker: An Automatic Detection Platform for Android GUI Errors}},
volume = {2018-Novem},
year = {2019}
}
@article{Mahmood2014b,
abstract = {Proliferation of Android devices and apps has created a demand for applicable automated software testing techniques. Prior research has primarily focused on either unit or GUI testing of Android apps, but not their end-to-end system testing in a systematic manner. We present EvoDroid, an evolutionary approach for system testing of Android apps. EvoDroid overcomes a key shortcoming of using evolutionary techniques for system testing, i.e., the inability to pass on genetic makeup of good individuals in the search. To that end, EvoDroid combines two novel techniques: (1) an Android-specific program analysis technique that identifies the segments of the code amenable to be searched independently, and (2) an evolutionary algorithm that given information of such segments performs a stepwise search for test cases reaching deep into the code. Our experiments have corroborated EvoDroid's ability to achieve significantly higher code coverage than existing Android testing tools.},
author = {Mahmood, Riyadh and Mirzaei, Nariman and Malek, Sam},
doi = {10.1145/2635868.2635896},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahmood, Mirzaei, Malek - 2014 - EvoDroid Segmented evolutionary testing of Android apps.pdf:pdf},
isbn = {9781450330565},
journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
keywords = {Android,Evolutionary testing,Program analysis},
pages = {599--609},
title = {{EvoDroid: Segmented evolutionary testing of Android apps}},
volume = {16-21-Nove},
year = {2014}
}
@article{Peng2020a,
author = {Peng, Chao and Rajan, Ajitha},
journal = {arXiv preprint arXiv:2011.11766},
title = {{CAT: Change-focused Android GUI Testing}},
year = {2020}
}
@inproceedings{Gu2019a,
author = {Gu, Tianxiao and Sun, Chengnian and Ma, Xiaoxing and Cao, Chun and Xu, Chang and Yao, Yuan and Zhang, Qirun and Lu, Jian and Su, Zhendong},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2019 - Practical GUI testing of Android applications via model abstraction and refinement.pdf:pdf},
pages = {269--280},
publisher = {IEEE Press},
title = {{Practical GUI testing of Android applications via model abstraction and refinement}},
year = {2019}
}
@inproceedings{Linares-Vasquez2015b,
abstract = {GUI-based models extracted from Android app execution traces, events, or source code can be extremely useful for challenging tasks such as the generation of scenarios or test cases. However, extracting effective models can be an expensive process. Moreover, existing approaches for automatically deriving GUI-based models are not able to generate scenarios that include events which were not observed in execution (nor event) traces. In this paper, we address these and other major challenges in our novel hybrid approach, coined as MONKEYLAB. Our approach is based on the Record→Mine→Generate→Validate framework, which relies on recording app usages that yield execution (event) traces, mining those event traces and generating execution scenarios using statistical language modeling, static and dynamic analyses, and validating the resulting scenarios using an interactive execution of the app on a real device. The framework aims at mining models capable of generating feasible and fully replayable (i.e., Actionable) scenarios reflecting either natural user behavior or uncommon usages (e.g., Corner cases) for a given app. We evaluated MONKEYLAB in a case study involving several medium-to-large open-source Android apps. Our results demonstrate that MONKEYLAB is able to mine GUI-based models that can be used to generate actionable execution scenarios for both natural and unnatural sequences of events on Google Nexus 7 tablets.},
archivePrefix = {arXiv},
arxivId = {1801.06271},
author = {Linares-V{\'{a}}squez, Mario and White, Martin and Bernal-C{\'{a}}rdenas, Carlos and Moran, Kevin and Poshyvanyk, Denys},
booktitle = {IEEE International Working Conference on Mining Software Repositories},
doi = {10.1109/MSR.2015.18},
eprint = {1801.06271},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Linares-V{\'{a}}squez et al. - 2015 - Mining android app usages for generating actionable GUI-based execution scenarios.pdf:pdf},
isbn = {9780769555942},
issn = {21601860},
keywords = {GUI models,Language models,Mining execution traces and event logs,Mobile app},
pages = {111--122},
publisher = {IEEE},
title = {{Mining android app usages for generating actionable GUI-based execution scenarios}},
volume = {2015-Augus},
year = {2015}
}
@inproceedings{White2015,
abstract = {Manually reproducing bugs is time-consuming and tedious. Software maintainers routinely try to reproduce unconfirmed issues using incomplete or no informative bug reports. Consequently, while reproducing an issue, the maintainer must augment the report with information - such as a reliable sequence of descriptive steps to reproduce the bug - to aid developers with diagnosing the issue. This process encumbers issue resolution from the time the bug is entered in the issue tracking system until it is reproduced. This paper presents Crash Droid, an approach for automating the process of reproducing a bug by translating the call stack from a crash report into expressive steps to reproduce the bug and a kernel event trace that can be replayed on-demand. Crash Droid manages trace ability links between scenarios' natural language descriptions, method call traces, and kernel event traces. We evaluated Crash Droid on several open-source Android applications infected with errors. Given call stacks from crash reports, Crash Droid was able to generate expressive steps to reproduce the bugs and automatically replay the crashes. Moreover, users were able to confirm the crashes faster with Crash Droid than manually reproducing the bugs or using a stress-testing tool.},
author = {White, Martin and Linares-V{\'{a}}squez, Mario and Johnson, Peter and Bernal-C{\'{a}}rdenas, Carlos and Poshyvanyk, Denys},
booktitle = {IEEE International Conference on Program Comprehension},
doi = {10.1109/ICPC.2015.14},
isbn = {9781467381598},
keywords = {Android,crash and bug reports,reproducibility},
pages = {48--59},
publisher = {IEEE},
title = {{Generating Reproducible and Replayable Bug Reports from Android Application Crashes}},
volume = {2015-Augus},
year = {2015}
}
@inproceedings{Sadeghi2017b,
abstract = { Recent introduction of a dynamic permission system in Android, allowing the users to grant and revoke permissions after the installation of an app, has made it harder to properly test apps. Since an app's behavior may change depending on the granted permissions, it needs to be tested under a wide range of permission combinations. At the state-of-the-art, in the absence of any automated tool support, a developer needs to either manually determine the interaction of tests and app permissions, or exhaustively re-execute tests for all possible permission combinations, thereby increasing the time and resources required to test apps. This paper presents an automated approach, called PATDroid, for efficiently testing an Android app while taking the impact of permissions on its behavior into account. PATDroid performs a hybrid program analysis on both an app under test and its test suite to determine which tests should be executed on what permission combinations. Our experimental results show that PATDroid significantly reduces the testing effort, yet achieves comparable code coverage and fault detection capability as exhaustively testing an app under all permission combinations. },
author = {Sadeghi, Alireza and Jabbarvand, Reyhaneh and Malek, Sam},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
doi = {10.1145/3106237.3106250},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sadeghi, Jabbarvand, Malek - 2017 - PATDroid permission-aware GUI testing of Android.pdf:pdf},
isbn = {1450351050},
pages = {220--232},
publisher = {ACM},
title = {{PATDroid: permission-aware GUI testing of Android}},
year = {2017}
}
@article{Choi2018a,
abstract = {In recent years, several automated GUI testing techniques for Android apps have been proposed. These tools have been shown to be effective in achieving good test coverage and in finding bugs without human intervention. Being automated, these tools typically run for a long time (say, for several hours), either until they saturate test coverage or until a testing time budget expires. Thus, these automated tools are not good at generating concise regression test suites that could be used for testing in incremental development of the apps and in regression testing. We propose a heuristic technique that helps create a small regression test suite for an Android app from a large test suite generated by an automated Android GUI testing tool. The key insight behind our technique is that if we can identify and remove some common forms of redundancies introduced by existing automated GUI testing tools, then we can drastically lower the time required to minimize a GUI test suite. We have implemented our algorithm in a prototype tool called DetReduce. We applied DetReduce to several Android apps and found that DetReduce reduces a test-suite by an average factor of 16.9× in size and 14.7× in running time. We also found that for a test suite generated by running SwiftHand and a randomized test generation algorithm for 8 hours, DetReduce minimizes the test suite in an average of 14.6 hours.},
author = {Choi, Wontae and Sen, Koushik and Necula, George and Wang, Wenyu},
doi = {10.1145/3180155.3180173},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi et al. - 2018 - DetReduce Minimizing Android GUI test suites for regression testing.pdf:pdf},
isbn = {9781450356381},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Android,Graphical User Interface,Test minimization},
pages = {445--455},
title = {{DetReduce: Minimizing Android GUI test suites for regression testing}},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{Chu2018,
abstract = {Automated GUI (graphical user interface) testing tools have been used to help engineers test whether the software GUI is displayed correctly in different smartphones. However, due to different screen aspect ratios, the ratio of width to height, the same content of a mobile application (app) may have a different layout in different smartphones. As a result, the test oracle generated by traditional methods may not be reused for different smartphones and thus prolong the testing process. In this paper, we present a GUI testing tool, named FLAG (Fully Automatic mobile GUI testing), which aims to make the test oracle reusable without compromising test accuracy. In addition, the whole testing process, including generating test cases, simulating user gestures and verifying results, is automatically performed by FLAG without human interaction. News applications have been selected for our study not only because they are popular, but also because they support most commonly-used user gestures, such as tap, scroll, spread and pinch. In our experiment, we selected five commercial Android phones and one popular news apps to evaluate the effectiveness of the FLAG. Our experiment results show that the FLAG performs better than existing methods and can achieve an average accuracy of 95.20% in determining whether a test has passed or failed.},
author = {Chu, Edward T.H. and Lin, Jun Yan},
booktitle = {Proceedings - 2018 International Symposium on Computer, Consumer and Control, IS3C 2018},
doi = {10.1109/IS3C.2018.00013},
isbn = {9781538670361},
keywords = {Android,GUI testing tools,Test case generator,Test oracle},
pages = {14--17},
publisher = {IEEE},
title = {{Automated GUI testing for android news applications}},
year = {2019}
}
@inproceedings{Yan2020,
abstract = {Existing GUI testing approaches of Android apps usually test apps from a single entry. In this way, the marginal activities far away from the default entry are difficult to be covered. The marginal activities may fail to be launched due to requiring a great number of activity transitions or involving complex user operations, leading to uneven coverage on activity components. Besides, since the test space of GUI programs is infinite, it is difficult to test activities under complete launching contexts using single-entry testing approaches. In this paper, we address these issues by constructing activity launching contexts and proposing a multiple-entry testing framework. We perform an inter-procedural, flow-, context- and pathsensitive analysis to build activity launching models and generate complete launching contexts. By activity exposing and static analysis, we could launch activities directly under various contexts without performing long event sequence on GUI. Besides, to achieve an in-depth exploration, we design an adaptive exploration framework which supports the multiple-entry exploration and dynamically assigns weights to entries in each turn. Our approach is implemented in a tool called Fax, with an activity launching strategy Faxla and an exploration strategy Faxex . The experiments on 20 real-world apps show that Faxla can cover 96.4% and successfully launch 60.6% activities, based on which Faxex further achieves a relatively 19.7% improvement on method coverage compared with the most popular tool Monkey. Our tool also behaves well in revealing hidden bugs. Fax can trigger over seven hundred unique crashes, including 180 Errors and 539 Warnings, which is significantly higher than those of other tools. Among the 46 bugs reported to developers on Github, 33 have been fixed up to now.},
author = {Yan, Jiwei and Liu, Hao and Pan, Linjie and Yan, Jun and Zhang, Jian and Liang, Bin},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/3377811.3380347},
isbn = {9781450371216},
issn = {02705257},
keywords = {Android,Icc,Multiple-entry testing,Static analysis},
pages = {457--468},
publisher = {IEEE},
title = {{Multiple-entry testing of android applications by constructing activity launching contexts}},
year = {2020}
}
@inproceedings{Coppola2016,
abstract = {Automated UI testing suffers from fragility due to continuous - although minor - changes in the UI of applications. Such fragility has been shown especially for the web domain, though no clear evidence is available for mobile applications. Our goal is to perform an exploratory assessment of the extent and causes of the fragiliy of UI automated tests for mobile applications. For this purpose, we analyzed a small test suite -that we developed using five different testing frameworks- for an Android application (K-9 Mail) and observed the changes induced in the tests by the evolution of the UI. We found that up to 75% of code-based tests, and up to 100% of image recognition tests, had to be adapted because of the changes induced by the evolution of the application between two different versions. In addition we identified the main causes of such fragility: changes of identifiers, text or graphics, removal or relocation of elements, activity ow variation, execution time variation, and usage of physical buttons. The preliminary assessment showed that the fragility of UI tests can be a relevant issue also for mobile applications. A few common causes were found that can be used as the basis for providing guidelines for fragility avoidance and repair.},
author = {Coppola, Riccardo and Raffero, Emanuele and Torchiano, Marco},
booktitle = {INTUITEST 2016 - Proceedings of the 2nd International Workshop on User Interface Test Automation, Co-located with ISSTA 2016},
doi = {10.1145/2945404.2945406},
isbn = {9781450344128},
keywords = {Automated,Empirical,Fragility,Graphical user interface,Test},
pages = {11--20},
publisher = {ACM},
title = {{Automated mobile UI test fragility: An exploratory assessment study on android}},
year = {2016}
}
@article{Yang2018,
abstract = {This work develops a static analysis to create a model of the behavior of an Android application's GUI. We propose the window transition graph (WTG), a model representing the possible GUI window sequences and their associated events and callbacks. A key component and contribution of our work is the careful modeling of the stack of currently-active windows, the changes to this stack, and the effects of callbacks related to these changes. To the best of our knowledge, this is the first detailed study of this important static analysis problem for Android. We develop novel analysis algorithms for WTG construction and traversal, based on this modeling of the window stack. We also propose WTG extensions to handle certain aspects of asynchronous control flow. We describe an application of the WTG for GUI test generation, using path traversals. The evaluation of the proposed algorithms indicates their effectiveness and practicality.},
author = {Yang, Shengqian and Wu, Haowei and Zhang, Hailong and Wang, Yan and Swaminathan, Chandrasekar and Yan, Dacong and Rountev, Atanas},
doi = {10.1007/s10515-018-0237-6},
issn = {15737535},
journal = {Automated Software Engineering},
keywords = {Android,GUI analysis,Static analysis},
number = {4},
pages = {833--873},
publisher = {Springer},
title = {{Static window transition graphs for Android}},
volume = {25},
year = {2018}
}
@inproceedings{Ki2017,
abstract = {We demonstrate AutoClicker, a fully automated UI testing system for large-scale Android apps using multiple devices. It provides a way to quickly and easily verify that a large number of Android apps behave correctly at runtime in a repeatable manner.},
author = {Ki, Taeyeon and Simeonov, Alexander and Park, Chang Min and Dantu, Karthik and Ko, Steven Y. and Ziarek, Lukasz},
booktitle = {MobiSys 2017 - Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services},
doi = {10.1145/3081333.3089330},
isbn = {9781450349284},
pages = {185},
publisher = {ACM},
title = {{Demo: Fully automated UI testing system for large-scale Android apps using multiple devices}},
year = {2017}
}
@inproceedings{Mirzaei2015,
abstract = {Pervasiveness of smartphones and the vast number of corresponding apps have underlined the need for applicable automated software testing techniques. A wealth of research has been focused on either unit or GUI testing of smartphone apps, but little on automated support for end-to-end system testing. This paper presents SIG-Droid, a framework for system testing of Android apps, backed with automated program analysis to extract app models and symbolic execution of source code guided by such models for obtaining test inputs that ensure covering each reachable branch in the program. SIG-Droid leverages two automatically extracted models: Interface Model and Behavior Model. The Interface Model is used to find values that an app can receive through its interfaces. Those values are then exchanged with symbolic values to deal with constraints with the help of a symbolic execution engine. The Behavior Model is used to drive the apps for symbolic execution and generate sequences of events. We provide an efficient implementation of SIG-Droid based in part on Symbolic PathFinder, extended in this work to support automatic testing of Android apps. Our experiments show SIG-Droid is able to achieve significantly higher code coverage than existing automated testing tools targeted for Android.},
author = {Mirzaei, Nariman and Bagheri, Hamid and Mahmood, Riyadh and Malek, Sam},
booktitle = {2015 IEEE 26th International Symposium on Software Reliability Engineering, ISSRE 2015},
doi = {10.1109/ISSRE.2015.7381839},
isbn = {9781509004065},
keywords = {Android,Automated Testing,Input Generation},
pages = {461--471},
publisher = {IEEE},
title = {{SIG-Droid: Automated system input generation for Android applications}},
year = {2016}
}
@inproceedings{Cao2018,
abstract = {This paper presents an effective model-based GUI testing technique for Android apps. To avoid local and repetitive exploration, our approach groups equivalent widgets in a state and designs a novel feedback-based exploration strategy, which dynamically adjusts the priority of actions based on the execution result of those already triggered ones, and tends to select actions that can reach news states of apps. We implemented our technique in a tool, called CrawlDroid, and conducted empirical experiments. Our results show that the proposed technique is effective, and covers more code within a fixed testing budget.},
author = {Cao, Yuzhong and Wu, Guoquan and Chen, Wei and Wei, Jun},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3275219.3275238},
isbn = {9781450365901},
keywords = {Graphical user interface,Mobile app,Model-based testing},
pages = {19},
publisher = {ACM},
title = {{CrawlDroid: Effective model-based GUI testing of android apps}},
year = {2018}
}
@inproceedings{Imparato2015,
abstract = {Mobile applications have become an integral part of the daily lives of millions of users, thus making necessary to ensure their security and reliability. Moreover the increasing number of mobile applications with rich Graphical User Interfaces (GUI) creates a growing need for automated techniques of GUI Testing for mobile applications. In this paper, the GUI Ripping Technique is combined with the Input Perturbation Testing to improve the quality of Android Application Testing. The proposed technique, based on a systematic and automatic exploration of the behavior of Android applications, creates a model of the explored GUI and then uses it to generate the perturbed text inputs. The technique was evaluated on many Android apps and its results were compared with random input tests.},
author = {Imparato, Gennaro},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.241},
isbn = {9781479919345},
issn = {02705257},
keywords = {Android,GUI Ripping,Input Perturbation Testing,Testing Automation},
pages = {760--762},
publisher = {IEEE Press},
title = {{A Combined Technique of GUI Ripping and Input Perturbation Testing for Android Apps}},
volume = {2},
year = {2015}
}
@inproceedings{Wen2015,
abstract = {Android is currently the most widely used operating system for mobile devices. GUI testing for Android applications becomes an important research area in which many studies have been conducted. The past studies show that testing a complicated GUI design may need a large number of test cases which increases exponentially due to the complexity of the GUI. Developers then need to spend a large amount of testing time in executing the test cases to explore the potential software defects. Unfortunately, the testing efficiency issue has not been comprehensively discussed in related studies. In this paper, we describe a parallel GUI testing platform called PATS (Parallel Android Testing System) which performs GUI testing based on a master-slave model. In PATS, the application under test is analyzed dynamically under the cooperation of the master and the slaves. Since the test cases are also generated in parallel at the runtime, the testing efficiency can be improved. We have implemented a prototype and conducted experiments with Android apps. The experimental results show that PATS can effectively improve the testing time with 18.87-35.78% performance improvements.},
author = {Wen, Hsiang Lin and Lin, Chia Hui and Hsieh, Tzong Han and Yang, Cheng Zen},
booktitle = {Proceedings - International Computer Software and Applications Conference},
doi = {10.1109/COMPSAC.2015.80},
isbn = {9781467365635},
issn = {07303157},
keywords = {Android,Fine-grained event sequences,GUI ripping,Parallel testing},
pages = {210--215},
publisher = {IEEE},
title = {{PATS: A Parallel GUI Testing Framework for Android Applications}},
volume = {2},
year = {2015}
}
@inproceedings{Baek2016,
author = {Baek, Young-Min and Bae, Doo-Hwan},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
isbn = {1450338453},
pages = {238--249},
publisher = {ACM},
title = {{Automated model-based Android GUI testing using multi-level GUI comparison criteria}},
year = {2016}
}
@inproceedings{Vuong2019,
abstract = {Since the big boom of smartphone and consequently of mobile applications, developers nowadays have many tools to help them create applications easier and faster. However, efficient automated testing tools are still missing, especially for GUI testing. We propose an automated GUI testing tool for Android applications using Deep Q-Network and semantic analysis of the GUI. We identify the semantic meanings of GUI elements and use them as an input to a neural network, which through training, approximates the behavioral model of the application under test. The neural network is trained using the Q-Learning algorithm of Reinforcement Learning. It guides the testing tool to explore more often functionalities that can only be accessed through a specific sequence of actions. The tool does not require access to the source code of the application under test. It obtains higher code coverage and is better at fault detection in comparison to state-of-the-art testing tools.},
author = {Vuong, Tuyet and Takada, Shingo},
booktitle = {Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
doi = {10.18293/SEKE2019-080},
isbn = {1891706489},
issn = {23259086},
keywords = {Automated testing,Deep Q-network,GUI testing,Reinforcement learning},
pages = {123--128},
publisher = {Knowledge Systems Institute Graduate School},
title = {{Semantic analysis for deep Q-network in android GUI testing}},
volume = {2019-July},
year = {2019}
}
@inproceedings{Mao2016,
abstract = {We introduce Sapienz, an approach to Android testing that uses multi-objective search-based testing to automatically explore and optimise test sequences, minimising length, while simultaneously maximising coverage and fault revelation. Sapienz combines random fuzzing, systematic and search-based exploration, exploiting seeding and multi-level instrumentation. Sapienz significantly outperforms (with large effect size) both the state-of-the-art technique Dynodroid and the widely-used tool, Android Monkey, in 7/10 experiments for coverage, 7/10 for fault detection and 10/10 for fault-revealing sequence length. When applied to the top 1, 000 Google Play apps, Sapienz found 558 unique, previously unknown crashes. So far we have managed to make contact with the developers of 27 crashing apps. Of these, 14 have confirmed that the crashes are caused by real faults. Of those 14, six already have developer-confirmed fixes.},
author = {Mao, Ke and Harman, Mark and Jia, Yue},
booktitle = {ISSTA 2016 - Proceedings of the 25th International Symposium on Software Testing and Analysis},
doi = {10.1145/2931037.2931054},
isbn = {9781450343909},
keywords = {Android,Search-based software testing,Test generation},
pages = {94--105},
publisher = {ACM},
title = {{Sapienz: Multi-objective automated testing for android applications}},
year = {2016}
}
@inproceedings{Li2017a,
abstract = {As many automated test input generation tools for Android need to instrument the system or the app, they cannot be used in some scenarios such as compatibility testing and malware analysis. We introduce DroidBot, a lightweight UI-guided test input generator, which is able to interact with an Android app on almost any device without instrumentation. The key technique behind DroidBot is that it can generate UI-guided test inputs based on a state transition model generated on-the-fly, and allow users to integrate their own strategies or algorithms. DroidBot is lightweight as it does not require app instrumentation, thus users do not need to worry about the inconsistency between the tested version and the original version. It is compatible with most Android apps, and able to run on almost all Android-based systems, including customized sandboxes and commodity devices. Droidbot is released as an open-source tool on GitHub, and the demo video can be found at https://youtu.be/3-aHG-SazMY.},
author = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
booktitle = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
doi = {10.1109/ICSE-C.2017.8},
isbn = {9781538615898},
keywords = {Android,Automated testing,Compatibility testing,Dynamic analysis,Malware detection},
pages = {23--26},
publisher = {IEEE},
title = {{DroidBot: A lightweight UI-guided test input generator for android}},
year = {2017}
}
@inproceedings{Qin2019,
abstract = {Nowadays, Apple iOS and Android are two most popular platforms for mobile applications. To attract more users, many software companies and organizations are migrating their applications from one platform to the other, and besides source files, they also need to migrate their GUI tests. The migration of GUI tests is tedious and difficult to be automated, since two platforms have subtle differences and there are often few or even no migrated GUI tests for learning. To address the problem, in this paper, we propose a novel approach, TestMig, that migrates GUI tests from iOS to Android, without any migrated code samples. Specifically, TestMig first executes the GUI tests of the iOS version, and records their GUI event sequences. Guided by the iOS GUI events, TestMig explores the Android version of the application to generate the corresponding Android event sequences. We conducted an evaluation on five well known mobile applications: 2048, SimpleNote, Wire, Wikipedia, and WordPress. The results show that, on average, TestMig correctly converts 80.2% of recorded iOS UI events to Android UI events and have them successfully executed, and our migrated Android test cases achieve similar statement coverage compared with the original iOS test cases (59.7% vs 60.4%).},
author = {Qin, Xue and Zhong, Hao and Wang, Xiaoyin},
booktitle = {ISSTA 2019 - Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
doi = {10.1145/3293882.3330575},
isbn = {9781450362245},
keywords = {GUI Testing,Mobile App,Test Migration},
pages = {329--340},
title = {{TestMIG: Migrating GUI test cases from iOS to android}},
year = {2019}
}
@inproceedings{Salihu2016,
abstract = {The popularity of mobile devices is ever increasing which led to rapid increase in the development of mobile applications. GUI testing has been an effective means of validating Android apps. However, it still suffers a strong challenge about how to explore event sequence in the GUIs. This paper proposes a hybrid approach for systematic exploration of mobile apps which exploit the capabilities of both static and dynamic approaches while trying to improve app's state exploration. Our approach is based static analysis on app's bytecode to extract events supported by an app. The generated events are used to dynamically explore an app at run-time. The experimental results show that our approach can explore significant number of app's state for the generation of high quality test case.},
author = {Salihu, Ibrahim Anka and Ibrahim, Rosziati},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3007120.3011072},
isbn = {9781450348065},
keywords = {Android,Code coverage,Dynamic analysis,GUI testing,Hybrid testing,Static analysis,Systematic exploration,Test case generation},
pages = {50--54},
publisher = {ACM},
title = {{Systematic exploration of android apps' events for automated testing}},
year = {2016}
}
@article{Amalfitano2012a,
abstract = {We present AndroidRipper, an automated technique that tests Android apps via their Graphical User Interface (GUI). AndroidRipper is based on a user-interface driven ripper that automatically explores the app's GUI with the aim of exercising the application in a structured manner. We evaluate AndroidRipper on an open-source Android app. Our results show that our GUI-based test cases are able to detect severe, previously unknown, faults in the underlying code, and the structured exploration outperforms a random approach. Copyright 2012 ACM.},
author = {Amalfitano, Domenico and Fasolino, Anna Rita and Tramontana, Porfirio and {De Carmine}, Salvatore and Memon, Atif M.},
doi = {10.1145/2351676.2351717},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amalfitano et al. - 2012 - Using GUI ripping for automated testing of android applications.pdf:pdf},
isbn = {9781450312042},
journal = {2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings},
keywords = {Android,Testing automation,Testing tools},
pages = {258--261},
title = {{Using GUI ripping for automated testing of android applications}},
year = {2012}
}
@article{Su2017b,
abstract = {Mobile apps are ubiquitous, operate in complex environments and are developed under the time-to-market pressure. Ensuring their correctness and reliability thus becomes an important challenge. This paper introduces Stoat, a novel guided approach to perform stochastic model-based testing on Android apps. Stoat operates in two phases: (1) Given an app as input, it uses dynamic analysis enhanced by a weighted UI exploration strategy and static analysis to reverse engineer a stochastic model of the app's GUI interactions; and (2) it adapts Gibbs sampling to iteratively mutate/refine the stochastic model and guides test generation from the mutated models toward achieving high code and model coverage and exhibiting diverse sequences. During testing, system-level events are randomly injected to further enhance the testing effectiveness. Stoat was evaluated on 93 open-source apps. The results show (1) the models produced by Stoat cover 17$\sim$31% more code than those by existing modeling tools; (2) Stoat detects 3X more unique crashes than two state-of-the-art testing tools, Monkey and Sapienz. Furthermore, Stoat tested 1661 most popular Google Play apps, and detected 2110 previously unknown and unique crashes. So far, 43 developers have responded that they are investigating our reports. 20 of reported crashes have been confirmed, and 8 already fixed.},
author = {Su, Ting and Meng, Guozhu and Chen, Yuting and Wu, Ke and Yang, Weiming and Yao, Yao and Pu, Geguang and Liu, Yang and Su, Zhendong},
doi = {10.1145/3106237.3106298},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Su et al. - 2017 - Guided, stochastic model-based GUI testing of android apps.pdf:pdf},
isbn = {9781450351058},
journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
keywords = {GUI testing,Mobile app,Model-based testing},
pages = {245--256},
title = {{Guided, stochastic model-based GUI testing of android apps}},
volume = {Part F1301},
year = {2017}
}
@inproceedings{Moran2016a,
author = {Moran, Kevin and Linares-V{\'{a}}squez, Mario and Bernal-C{\'{a}}rdenas, Carlos and Vendome, Christopher and Poshyvanyk, Denys},
booktitle = {2016 IEEE international conference on software testing, verification and validation (icst)},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moran et al. - 2016 - Automatically Discovering, Reporting and Reproducing Android Application Crashes.pdf:pdf},
isbn = {1509018271},
pages = {33--44},
publisher = {IEEE},
title = {{Automatically discovering, reporting and reproducing android application crashes}},
year = {2016}
}
@inproceedings{Borges2018,
abstract = {Android applications (apps) represent an ever increasing portion of the software market. Automated test input generators are the state of the art for testing and security analysis. We introduce DroidMate-2 (DM-2), a platform to easily assist both developers and researchers to customize, develop and test new test generators. DM-2 can be used without app instrumentation or operating system modifications, as a test generator on real devices and emulators for app testing or regression testing. Additionally, it provides sensitive resource monitoring or blocking capabilities through a lightweight app instrumentation, out-of-the-box statement coverage measurement through a fully-fledged app instrumentation and native experiment reproducibility. In our experiments we compared DM-2 against DroidBot, a state-of-the-art test generator by measuring statement coverage. Our results show that DM-2 reached 96% of its peak coverage in less than 2/3 of the time needed by DroidBot, allowing for better and more efficient tests. On short runs (5 minutes) DM-2 outperformed DroidBot by 7% while in longer runs (1 hour) this difference increases to 8%. ACM DL Artifact: https://www.doi.org/10.1145/3264864. For the details see: https://github.com/uds-se/droidmate/wiki/ASE-2018:-Data.},
author = {Borges, Nataniel P. and Hotzkow, Jenny and Zeller, Andreas},
booktitle = {ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
doi = {10.1145/3238147.3240479},
isbn = {9781450359375},
keywords = {Android,Dynamic analysis,Test generation},
pages = {916--919},
publisher = {IEEE},
title = {{Droidmate-2: A platform for android test generation}},
year = {2018}
}
@inproceedings{Hu2011,
abstract = {Users increasingly rely on mobile applications for computational needs. Google Android is a popular mobile platform, hence the reliability of Android applications is becoming increasingly important. Many Android correctness issues, however, fall outside the scope of traditional verification techniques, as they are due to the novelty of the platform and its GUI-oriented application construction paradigm. In this paper we present an approach for automating the testing process for Android applications, with a focus on GUI bugs. We first conduct a bug mining study to understand the nature and frequency of bugs affecting Android applications; our study finds that GUI bugs are quite numerous. Next, we present techniques for detecting GUI bugs by automatic generation of test cases, feeding the application random events, instrumenting the VM, producing log/trace files and analyzing them post-run. We show how these techniques helped to re-discover existing bugs and find new bugs, and how they could be used to prevent certain bug categories. We believe our study and techniques have the potential to help developers increase the quality of Android applications. {\textcopyright} 2011 ACM.},
author = {Hu, Cuixiong and Neamtiu, Iulian},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/1982595.1982612},
isbn = {9781450305853},
issn = {02705257},
keywords = {android,empirical bug studies,gui testing,mobile applications,test automation,test case generation},
pages = {77--83},
publisher = {ACM},
title = {{Automating GUI testing for android applications}},
year = {2011}
}
@inproceedings{Su2016,
abstract = {GUI testing has been an effective means of validating Android apps. Meanwhile, it still faces a strong challenge about how to explore trails, i.e., unfrequented test sequences, as defects tend to reside on these unfrequented trails. This paper introduces FSMdroid, a novel, guided approach to GUI testing of Android apps. The essential idea of FSMdroid is to (1) construct an initial stochastic model for the app under test, (2) iteratively mutate the stochastic model and derive tests. The model mutations are guided by an MCMC sampling method such that the resulting test sequences can be diverse and also achieve high code coverage during testing. We have evaluated FSMdroid on 40 real-world Android apps. Compared with the traditional model-based testing approaches, FSMdroid enhances the diversity of test sequences by 85%, but reduces the number of them by 54%. Furthermore, we uncover 7 app bugs.},
author = {Su, Ting},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/2889160.2891043},
isbn = {9781450341615},
issn = {02705257},
pages = {689--691},
publisher = {IEEE},
title = {{FSMdroid: Guided GUI testing of android apps}},
year = {2016}
}
@inproceedings{Wu2018,
abstract = {Due to the widespread use of Android devices and apps, it is important to develop tools and techniques to improve app quality and performance. Our work focuses on a problem related to hardware sensors on Android devices: the failure to disable unneeded sensors, which leads to sensor leaks and thus battery drain. We propose the Sentinel testing tool to uncover such leaks. The tool performs static analysis of app code and produces a model which maps GUI events to callback methods that affect sensor behavior. Edges in the model are labeled with symbols representing the acquiring/releasing of sensors and the opening/closing of UI windows. The model is traversed to identify paths that are likely to exhibit sensor leaks during run-time execution based on two context-free languages over the symbol alphabet. The reported paths are then used to generate test cases. The execution of each test case tracks the run-time behavior of sensors and reports observed leaks. This approach has been applied to both open-sourced and closed-sourced regular Android applications as well as watch faces for Android Wear smartwatches. Our experimental results indicate that Sentinel effectively detects sensor leaks, while focusing the testing efforts on a very small subset of possible GUI event sequences.},
author = {Wu, Haowei and Zhang, Hailong and Wang, Yan and Rountev, Atanas},
booktitle = {Software Quality Journal},
doi = {10.1007/s11219-019-09484-z},
isbn = {1450357431},
issn = {15731367},
keywords = {Android,Android Wear,Energy,Graphical User Interface,Sensor,Smartwatch,Static analysis,Testing},
number = {1},
pages = {335--367},
publisher = {IEEE},
title = {{Sentinel: generating GUI tests for sensor leaks in Android and Android wear apps}},
volume = {28},
year = {2020}
}
@inproceedings{Mirzaei2016,
abstract = {The rising popularity of Android and the GUI-driven nature of its apps have motivated the need for applicable automated GUI testing techniques. Although exhaustive testing of all possible combinations is the ideal upper bound in combinatorial testing, it is often infeasible, due to the combinatorial explosion of test cases. This paper presents TrimDroid, a framework for GUI testing of Android apps that uses a novel strategy to generate tests in a combinatorial, yet scalable, fashion. It is backed with automated program analysis and formally rigorous test generation engines. TrimDroid relies on program analysis to extract formal specifications. These speci-fications express the app's behavior (i.e., control flow between the various app screens) as well as the GUI elements and their dependencies. The dependencies among the GUI elements comprising the app are used to reduce the number of combinations with the help of a solver. Our experiments have corroborated TrimDroid's ability to achieve a comparable coverage as that possible under exhaustive GUI testing using significantly fewer test cases.},
author = {Mirzaei, Nariman and Garcia, Joshua and Bagheri, Hamid and Sadeghi, Alireza and Malek, Sam},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/2884781.2884853},
isbn = {9781450339001},
issn = {02705257},
keywords = {Android,Input Generation,Software Testing},
pages = {559--570},
publisher = {IEEE},
title = {{Reducing combinatorics in GUI testing of android applications}},
volume = {14-22-May-},
year = {2016}
}
@inproceedings{Wang2020e,
abstract = {Android apps demand high-quality test inputs, whose generation remains an open challenge. Existing techniques fall short on exploring complex app functionalities reachable only by a long, meaningful, and effective test input. Observing that such test inputs can usually be decomposed into relatively independent short use cases, this paper presents ComboDroid, a fundamentally different Android app testing framework. ComboDroid obtains use cases for manifesting a specific app functionality (either manually provided or automatically extracted), and systematically enumerates the combinations of use cases, yielding high-quality test inputs. The evaluation results of ComboDroid on real-world apps are encouraging. Our fully automatic variant outperformed the best existing technique APE by covering 4.6% more code (APE only outperformed Monkey by 2.1%), and revealed four previously unknown bugs in extensively tested subjects. Our semi-automatic variant boosts the manual use cases obtained with little manual labor, achieving a comparable coverage (only 3.2% less) with a white-box human testing expert.},
author = {Wang, Jue and Jiang, Yanyan and Xu, Chang and Cao, Chun and Ma, Xiaoxing and Lu, Jian},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/3377811.3380382},
isbn = {9781450371216},
issn = {02705257},
keywords = {Mobile app,Software testing},
pages = {469--480},
title = {{Combodroid: Generating high-quality test inputs for android apps via use case combinations}},
year = {2020}
}
@inproceedings{Wang2014,
abstract = {Android apps are usually rich in GUIs and users interact with the GUIs to use the functions provided by an app. To make Android apps reliable, GUI testing is an effective method. Automation and high GUI coverage is necessary in the testing for the sake of minimizing human effort and maximizing effectiveness. However, the existing work is insufficient to meet such requirements. In this paper, we identify several challenges for conducting GUI traversal on Android, such as component recognition, event injection and UI traversal. We present a tool named Droid Crawle to address the challenges for automatically exploring the GUIs of Android apps with high GUI coverage. The evaluation of Droid Crawler shows that it is efficient and effective to automatically capture the GUI tree of the target application with high GUI coverage. {\textcopyright} 2014 IEEE.},
author = {Wang, Peng and Liang, Bin and You, Wei and Li, Jingzhe and Shi, Wenchang},
booktitle = {Proceedings - 2014 4th International Conference on Communication Systems and Network Technologies, CSNT 2014},
doi = {10.1109/CSNT.2014.236},
isbn = {9781479930708},
keywords = {Android,Automatic GUI Testing,High coverage},
pages = {1161--1166},
publisher = {IEEE},
title = {{Automatic android GUI traversal with high coverage}},
year = {2014}
}
@inproceedings{Choi2013,
author = {Choi, Wontae and Necula, George and Sen, Koushik},
booktitle = {Acm Sigplan Notices},
isbn = {145032374X},
number = {10},
pages = {623--640},
publisher = {ACM},
title = {{Guided gui testing of android apps with minimal restart and approximate learning}},
volume = {48},
year = {2013}
}
@inproceedings{Borges2018a,
author = {Borges, Nataniel P and G{\'{o}}mez, Maria and Zeller, Andreas},
booktitle = {2018 IEEE/ACM 5th International Conference on Mobile Software Engineering and Systems (MOBILESoft)},
isbn = {1450357121},
pages = {133--143},
publisher = {IEEE},
title = {{Guiding app testing with mined interaction models}},
year = {2018}
}
@inproceedings{Li2014,
abstract = {Under the increasing complexity and time-to-market pressures, functional validation is becoming a major bottleneck of smartphone applications running on mobile platforms (e.g., Android, iOS). Due to the GUI (Graphical User Interface) intensive nature, the execution of smartphone applications heavily relies on the interactions with users. Manual GUI testing is extremely slow and unacceptably expensive in practice. However, the lack of formal models of user behaviors in the design phase hinders the automation of GUI testing (i.e., test case generation and test evaluation). While thorough test efforts are required to ensure the consistency between user behavior specifications and GUI implementations, few of existing testing approaches can automatically utilize the design phase information to test complex smartphone applications. Based on UML activity diagrams, this paper proposes an automated GUI testing framework called AD Automation, which supports user behavior modeling, GUI test case generation, and post-test analysis and debugging. The experiments using two industrial smartphone applications demonstrate that our approach can not only drastically reduce overall testing time, but also improve the quality of designs.},
author = {Li, Ang and Qin, Zishan and Chen, Mingsong and Liu, Jing},
booktitle = {Proceedings - 8th International Conference on Software Security and Reliability, SERE 2014},
doi = {10.1109/SERE.2014.20},
isbn = {9781479942961},
keywords = {Activity diagram,GUI testing,Smartphone applications},
pages = {68--77},
publisher = {IEEE},
title = {{ADAutomation: An activity diagram based automated GUI testing framework for smartphone applications}},
year = {2014}
}
@inproceedings{Zaeem2014,
abstract = {As the use of mobile devices becomes increasingly ubiquitous, the need for systematically testing applications (apps) that run on these devices grows more and more. However, testing mobile apps is particularly expensive and tedious, often requiring substantial manual effort. While researchers have made much progress in automated testing of mobile apps during recent years, a key problem that remains largely untracked is the classic oracle problem, i.e., to determine the correctness of test executions. This paper presents a novel approach to automatically generate test cases, that include test oracles, for mobile apps. The foundation for our approach is a comprehensive study that we conducted of real defects in mobile apps. Our key insight, from this study, is that there is a class of features that we term user-interaction features, which is implicated in a significant fraction of bugs and for which oracles can be constructed - in an application agnostic manner - based on our common understanding of how apps behave. We present an extensible framework that supports such domain specific, yet application agnostic, test oracles, and allows generation of test sequences that leverage these oracles. Our tool embodies our approach for generating test cases that include oracles. Experimental results using 6 Android apps show the effectiveness of our tool in finding potentially serious bugs, while generating compact test suites for user-interaction features. {\textcopyright} 2014 IEEE.},
author = {Zaeem, Razieh Nokhbeh and Prasad, Mukul R. and Khurshid, Sarfraz},
booktitle = {Proceedings - IEEE 7th International Conference on Software Testing, Verification and Validation, ICST 2014},
doi = {10.1109/ICST.2014.31},
isbn = {9780769551852},
pages = {183--192},
publisher = {IEEE},
title = {{Automated generation of oracles for testing user-interaction features of mobile apps}},
year = {2014}
}
@inproceedings{Borges2020,
abstract = {When generating GUI tests for Android apps, it typically is a separate test computer that generates interactions, which are then executed on an actual Android device. While this approach is efficient in the sense that apps and interactions execute quickly, the communication overhead between test computer and device slows down testing considerably. In this work, we present DD-2, a test generator for Android that tests other apps on the device using Android accessibility services. In our experiments, DD-2 has shown to be 3.2 times faster than its computer-device counterpart, while sharing the same source code.},
author = {Borges, Nataniel P. and Rau, Jenny and Zeller, Andreas},
booktitle = {Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020},
doi = {10.1145/3324884.3415302},
isbn = {9781450367684},
keywords = {Android,Dynamic analysis,test generation},
pages = {1340--1343},
publisher = {IEEE},
title = {{Speeding up GUI Testing by On-Device Test Generation}},
year = {2020}
}
@inproceedings{Yang2013,
abstract = {As the mobile platform continues to pervade all aspects of human activity, and mobile applications, or mobile apps for short, on this platform tend to be faulty just like other types of software, there is a growing need for automated testing techniques for mobile apps. Modelbased testing is a popular and important testing approach that operates on a model of an app's behavior. However, such a model is often not available or of insufficient quality. To address this issue, we present a novel grey-box approach for automatically extracting a model of a given mobile app. In our approach, static analysis extracts the set of events supported by the Graphical User Interface (GUI) of the app. Then dynamic crawling reverse-engineers a model of the app, by systematically exercising these events on the running app. We also present a tool implementing this approach for the Android platform. Our empirical evaluation of this tool on several Android apps demonstrates that it can efficiently extract compact yet reasonably comprehensive models of high quality for such apps. {\textcopyright} 2013 Springer-Verlag.},
author = {Yang, Wei and Prasad, Mukul R. and Xie, Tao},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-37057-1_19},
isbn = {9783642370564},
issn = {03029743},
pages = {250--265},
publisher = {Springer},
title = {{A grey-box approach for automated GUI-model generation of mobile applications}},
volume = {7793 LNCS},
year = {2013}
}
@inproceedings{Karlsson2021,
abstract = {Automatic testing of mobile applications has been a well-researched area in recent years. However, testing in industry is still a very manual practice, as research results have not been fully transferred and adopted. Considering mobile applications, manual testing has the additional burden of adequate testing posed by a large number of available devices and different configurations, as well as the maintenance and setup of such devices. In this paper, we propose and evaluate the use of a model-based test generation approach, where generated tests are executed on a set of cloud-hosted real mobile devices. By using a model-based approach we generate dynamic, less brittle, and implementation simple test cases. The test execution on multiple real devices with different configurations increase the confidence in the implementation of the system under test. Our evaluation shows that the used approach produces a high coverage of the parts of the application related to user interactions. Nevertheless, the inclusion of external services in test generation is required in order to additionally increase the coverage of the complete application. Furthermore, we present the lessons learned while transferring and implementing this approach in an industrial context and applying it to the real product.},
archivePrefix = {arXiv},
arxivId = {2008.08859},
author = {Karlsson, Stefan and {\v{C}}au{\v{s}}evi{\'{c}}, Adnan and Sundmark, Daniel and Larsson, M{\aa}rten},
booktitle = {2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
doi = {10.1109/icstw52544.2021.00033},
eprint = {2008.08859},
isbn = {1665444568},
pages = {130--137},
publisher = {IEEE},
title = {{Model-based Automated Testing of Mobile Applications: An Industrial Case Study}},
url = {http://arxiv.org/abs/2008.08859},
year = {2020}
}
@inproceedings{Zhao2020,
abstract = {UI testing is tedious and time-consuming due to the manual effort required. Recent research has explored opportunities for reusing existing UI tests from an app to automatically generate new tests for other apps. However, the evaluation of such techniques currently remains manual, unscalable, and unreproducible, which can waste effort and impede progress in this emerging area. We introduce FrUITeR, a framework that automatically evaluates UI test reuse in a reproducible way. We apply FrUITeR to existing test-reuse techniques on a uniform benchmark we established, resulting in 11,917 test reuse cases from 20 apps. We report several key findings aimed at improving UI test reuse that are missed by existing work.},
archivePrefix = {arXiv},
arxivId = {2008.03427},
author = {Zhao, Yixue and Chen, Justin and Sejfia, Adriana and {Schmitt Laser}, Marcelo and Zhang, Jie and Sarro, Federica and Harman, Mark and Medvidovic, Nenad},
booktitle = {ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
doi = {10.1145/3368089.3409708},
eprint = {2008.03427},
isbn = {9781450370431},
keywords = {Mobile App,Open Science,Software Testing,Test Reuse},
pages = {1190--1201},
title = {{FrUITeR: A framework for evaluating UI test reuse}},
year = {2020}
}
@inproceedings{Canny2020,
abstract = {The testing of applications with a Graphical User Interface (GUI) is a complex activity because of the infinity of possible event sequences. In the field of GUI Testing, model-based approaches based on reverse engineering of GUI application have been proposed to generate test cases. Unfortunately, evidences show that these techniques do not support some of the features of modern GUI applications. These features include dynamic widgets instantiation or advanced interaction techniques (e.g. multitouch). In this paper, we propose to build models of the applications from requirements, as it is standard practice in Model-Based Testing. To do so, we identified ICO (Interactive Cooperative Object) as one of the modelling techniques allowing the description of complex GUI behavior. We demonstrate that this notation is suitable for generating test cases targeting complex GUI applications in a process derived from the standard ModelBased Testing process.},
author = {Canny, Alexandre and Palanque, Philippe and Navarre, David},
booktitle = {Proceedings - 2020 IEEE 13th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2020},
doi = {10.1109/ICSTW50294.2020.00029},
isbn = {9781728110752},
keywords = {GUI Testing,Model-Based Testing,User Interface Description Languages},
pages = {95--104},
publisher = {IEEE},
title = {{Model-Based Testing of GUI Applications Featuring Dynamic Instanciation of Widgets}},
year = {2020}
}
@inproceedings{Hao2014,
abstract = {Mobile app ecosystems have experienced tremendous growth in the last six years. This has triggered research on dynamic analysis of performance, security, and correctness properties of the mobile apps in the ecosystem. Exploration of app execution using automated UI actions has emerged as an important tool for this research. However, existing research has largely developed analysis-specific UI automation techniques, wherein the logic for exploring app execution is intertwined with the logic for analyzing app properties. PUMA is a programmable framework that separates these two concerns. It contains a generic UI automation capability (often called a Monkey) that exposes high-level events for which users can define handlers. These handlers can flexibly direct the Monkey's exploration, and also specify app instrumentation for collecting dynamic state information or for triggering changes in the environment during app execution. Targeted towards operators of app marketplaces, PUMA incorporates mechanisms for scaling dynamic analysis to thousands of apps. We demonstrate the capabilities of PUMA by analyzing seven distinct performance, security, and correctness properties for 3,600 apps downloaded from the Google Play store. {\textcopyright} 2014 ACM.},
author = {Hao, Shuai and Liu, Bin and Nath, Suman and Halfond, William G.J. and Govindan, Ramesh},
booktitle = {MobiSys 2014 - Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services},
doi = {10.1145/2594368.2594390},
isbn = {9781450327930},
keywords = {dynamic analysis,large scale,mobile app,programming framework,separation of concerns,ui-automation},
pages = {204--217},
publisher = {ACM},
title = {{PUMA: Programmable UI-automation for large-scale dynamic analysis of mobile apps}},
year = {2014}
}
@inproceedings{Jensen2013,
abstract = {Automated software testing aims to detect errors by producing test inputs that cover as much of the application source code as possible. Applications for mobile devices are typically event-driven, which raises the challenge of automatically producing event sequences that result in high coverage. Some existing approaches use random or model-based testing that largely treats the application as a black box. Other approaches use symbolic execution, either starting from the entry points of the applications or on specific event sequences. A common limitation of the existing approaches is that they often fail to reach the parts of the application code that require more complex event sequences. We propose a two-phase technique for automatically finding event sequences that reach a given target line in the application code. The first phase performs concolic execution to build summaries of the individual event handlers of the application. The second phase builds event sequences backward from the target, using the summaries together with a UI model of the application. Our experiments on a collection of open source Android applications show that this technique can successfully produce event sequences that reach challenging targets. {\textcopyright} 2013 ACM.},
author = {Jensen, Casper S. and Prasad, Mukul R. and M{\o}ller, Anders},
booktitle = {2013 International Symposium on Software Testing and Analysis, ISSTA 2013 - Proceedings},
doi = {10.1145/2483760.2483777},
isbn = {9781450321594},
keywords = {Android,Symbolic execution,mobile app,test generation},
pages = {67--77},
publisher = {ACM},
title = {{Automated testing with targeted event sequence generation}},
year = {2013}
}
@article{Ali2018,
abstract = {Recently, testing mobile applications is gaining much attention due to the widespread of smartphones and the tremendous number of mobile applications development. It is essential to test mobile applications before being released for the public use. Graphical user interface (GUI) testing is a type of mobile applications testing conducted to ensure the proper functionality of the GUI components. Typically, GUI testing requires a lot of effort and time whether manual or automatic. Cloud computing is an emerging technology that can be used in the software engineering field to overcome the defects of the traditional testing approaches by using cloud computing resources. As a result, testing-as-a-service is introduced as a service model that conducts all testing activities in a fully automated manner. In this paper, a system for mobile applications GUI testing based on testing-as-a-service architecture is proposed. The proposed system performs all testing activities including automatic test case generation and simultaneous test execution on multiple virtual nodes for testing Android-based applications. The proposed system reduces testing time and meets fast time-to market constraint of mobile applications. Moreover, the proposed system architecture addresses many issues such as maximizing resource utilization, continuous monitoring to ensure system reliability, and applying fault-tolerance approach to handle occurrence of any failure.},
author = {Ali, Amira and Maghawry, Huda Amin and Badr, Nagwa},
doi = {10.1002/smr.1963},
issn = {20477481},
journal = {Journal of Software: Evolution and Process},
keywords = {Appium,GUI testing,TaaS,cloud computing,mobile app},
number = {10},
pages = {e1963},
publisher = {Wiley Online Library},
title = {{Automated parallel GUI testing as a service for mobile applications}},
volume = {30},
year = {2018}
}
@inproceedings{Li2017,
abstract = {The importance of regression testing in assuring the integrity of a program after changes is well recognized. One major obstacle in practicing regression testing is in maintaining tests that become obsolete due to evolved program behavior or specification. For mobile apps, the problem of maintaining obsolete GUI test scripts for regression testing is even more pressing. Mobile apps rely heavily on the correct functioning of their GUIs to compete on the market and provide good user experiences. But on the one hand, GUI tests break easily when changes happen to the GUI, On the other hand, mobile app developers often need to fight for a tight feedback loop and are left with limited time for test maintenance. In this paper, we propose a novel approach, called ATOM, to automatically maintain GUI test scripts of mobile apps for regression testing. ATOM uses an event sequence model to abstract possible event sequences on a GUI and a delta ESM to abstract the changes made to the GUI. Given both models as input, ATOM automatically updates the test scripts written for a base version app to reflect the changes. In an experiment with 22 versions from 11 production Android apps, ATOM updated all the test scripts affected by the version change, the updated scripts achieve over 80% of the coverage by the original scripts on the base version app, all except one set of updated scripts preserve over 60% of the actions in the original test scripts.},
author = {Li, Xiao and Chang, Nana and Wang, Yan and Huang, Haohua and Pei, Yu and Wang, Linzhang and Li, Xuandong},
booktitle = {Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation, ICST 2017},
doi = {10.1109/ICST.2017.22},
isbn = {9781509060313},
pages = {161--171},
publisher = {IEEE},
title = {{ATOM: Automatic Maintenance of GUI Test Scripts for Evolving Mobile Applications}},
year = {2017}
}
@inproceedings{Wetzlmaier2016,
abstract = {Testing via graphical user interfaces (GUI) is a complex and labor-intensive task. Numerous techniques, tools and frameworks have been proposed for automating GUI testing. In many projects, however, the introduction of automated tests did not reduce the overall effort of testing but shifted it from manual test execution to test script development and maintenance. As a pragmatic solution, random testing approaches (aka «monkey testing») have been suggested for automated random exploration of the system under test via the GUI. This paper presents a versatile framework for monkey GUI testing. The framework provides reusable components and a predefined, generic workflow with extension points for developing custom-built test monkeys. It supports tailoring the monkey for a particular application scenario and the technical requirements imposed by the system under test. The paper describes the customization of test monkeys for an open source project and in an industry application, where the framework has been used for successfully transferring the idea of monkey testing into an industry solution.},
author = {Wetzlmaier, Thomas and Ramler, Rudolf and Putschogl, Werner},
booktitle = {Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation, ICST 2016},
doi = {10.1109/ICST.2016.51},
isbn = {9781509018260},
keywords = {GUI testing,random testing,test automation tool},
pages = {416--423},
publisher = {IEEE},
title = {{A Framework for Monkey GUI Testing}},
year = {2016}
}
@inproceedings{Liu2020a,
abstract = {GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on difference devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot.Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4, 470 GUI screenshots with UI display issues. We develop a heuristics-based data augmentation method and a GAN-based data augmentation method for boosting the performance of our OwlEye. At present, the evaluation demonstrates that our OwlEye can achieve 85% precision and 84% recall in detecting UI display issues, and 90% accuracy in localizing these issues.},
author = {Liu, Zhe},
booktitle = {Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020},
doi = {10.1145/3324884.3418917},
isbn = {9781450367684},
keywords = {Deep Learning,Mobile App,UI display,UI testing},
pages = {1373--1375},
publisher = {IEEE},
title = {{Discovering UI Display Issues with Visual Understanding}},
year = {2020}
}
@inproceedings{Pan2019,
abstract = {Graphical User Interface (GUI) testing has been the focus of mobile app testing. Manual test cases, containing valuable human knowledge about the apps under test, are often coded as scripts to enable automated and repeated execution for test cost reduction. Unfortunately, many test scripts may become broken due to changes made during app updates. Broken test scripts are expected to be updated for reuse; however, the maintenance cost can be high if large numbers of test scripts require manual repair. We propose an approach named METER to repairing broken test scripts automatically when mobile apps are updated. METER novelly leverages computer vision techniques to infer GUI changes between two versions from screenshots and uses the GUI changes to guide the repair of test scripts. In experiments conducted on 18 Android apps, METER was able to repair 78.3% broken test scripts.},
author = {Pan, Minxue and Xu, Tongtong and Pei, Yu and Li, Zhong and Zhang, Tian and Li, Xuandong},
booktitle = {Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: Companion, ICSE-Companion 2019},
doi = {10.1109/ICSE-Companion.2019.00137},
isbn = {9781728117645},
keywords = {Computer vision,GUI testing,Mobile apps,OCR,Test script repair},
pages = {326--327},
publisher = {IEEE Press},
title = {{GUI-guided repair of mobile test scripts}},
year = {2019}
}
@inproceedings{Hasselknippe2017,
abstract = {As mobile apps are expected to run in many different screen sizes, the need to validate the correct positioning and rendering of graphical user interface (GUI) elements in such screens is increasing. In this paper, we first focus on identifying typical layout errors in modern GUIs and categorising them. Then, we implement a tool called Layout Bug Hunter (LBH) to automatically identify if a GUI layout is rendered correctly in different screen sizes. The tool is evaluated on mobile apps and is compared to state-of-the-art layout-testing tools. Results show that LBH can identify all the typical layout errors we categorise, and LBH is more accurate than a layout-testing tool based on image diffing algorithms. In addition, LBH does not require writing layout test scripts manually. LBH is currently implemented on only one mobile app development platform, but it is designed to be portable and extensible. With only limited effort, LBH can be extended to other mobile and web development platforms for layout testing.},
author = {Hasselknippe, Kristian Fjeld and Li, Jingyue},
booktitle = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
doi = {10.1109/APSEC.2017.87},
isbn = {9781538636817},
issn = {15301362},
keywords = {Graphical user interface,software testing,software tools},
pages = {695--700},
publisher = {IEEE},
title = {{A Novel Tool for Automatic GUI Layout Testing}},
volume = {2017-Decem},
year = {2018}
}
@article{Amalfitano2014,
abstract = {As mobile devices become increasingly smarter and more powerful, so too must the engineering of their software. User-interface-driven system testing of these devices is gaining popularity, with each vendor releasing some automation tool. However, these tools are inappropriate for amateur programmers, an increasing portion of app developers. MobiGUITAR (Mobile GUI Testing Framework) provides automated GUI-driven testing of Android apps. It's based on observation, extraction, and abstraction of GUI widgets' run-time state. The abstraction is a scalable state machine model that, together with test coverage criteria, provides a way to automatically generate test cases. When applied to four open-source Android apps, MobiGUITAR automatically generated and executed 7,711 test cases and reported 10 new bugs. Some bugs were Android-specific, stemming from the event-and activity-driven nature of Android.},
author = {Amalfitano, Domenico and Fasolino, Anna Rita and Tramontana, Porfirio and Ta, Bryan Dzung and Memon, Atif M.},
doi = {10.1109/MS.2014.55},
issn = {07407459},
journal = {IEEE Software},
keywords = {Android,GUI testing,MobiGUITAR,android testing,mobile app,software engineering,software testing},
number = {5},
pages = {53--59},
publisher = {IEEE},
title = {{MobiGUITAR: Automated Model-Based Testing of Mobile Apps}},
volume = {32},
year = {2015}
}
@inproceedings{Yamamoto2018,
abstract = {This paper is aimed at improving mobile game non-native GUI testing. We follow an approach to use OpenCV image recognition algorithms for detecting and accessing the hand-drawn GUI elements on the screen, in order to interact with them from within automated test scripts. In the previous work, we experienced the problem that some tests fail not due to the defects of the tested software itself, but because of the false positive results of template matching. It means that the high scores are sometimes elicited for the best match, though the requested GUI element is actually not present on the screen. In this contribution we investigate the possibilities of image filtering in order to reduce the number of such false positive cases. We describe our experiments with two algorithms supported by OpenCV library, a selection of GUI elements and mobile game scenes, and a number of image filtering methods. We demonstrate that using Canny edge detection filters can significantly improve the accuracy of recognizing false positive cases without affecting the true positive situations. Our conclusions can be helpful for improving hand-drawn GUI based mobile software testing reliability.},
author = {Yamamoto, Masato and Pyshkin, Evgeny and Mozgovoy, Maxim},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3274856.3274865},
isbn = {9781450365161},
keywords = {False positive,Image recognition,Non-native GUI,Software testing},
pages = {41--45},
publisher = {ACM},
title = {{Reducing false positives in automated OpenCV-based non-native GUI software testing}},
year = {2018}
}
@inproceedings{Cai2020,
abstract = {Model-based test (MBT) generation techniques for automated GUI testing are of great value for app testing. Existing GUI model-based testing tools may fall into cyclic operations and run out of resources, when applied to apps with industrial complexity and scalability. In this work, we present a multi-agent GUI MBT system named Fastbot. Fastbot performs model construction on the server end. It applies multi-agent collaboration mechanism to speed up the model construction procedure. The proposed approach was applied on more than 20 applications from Bytedance with more than 1500 million monthly active users. Higher code coverage in less testing time is achieved with comparison of three other automated testing tools including Droidbot, Humanoid and Android Monkey. CCS CONCEPTS • Software and its engineering → Software testing and debugging; Model-based software engineering},
author = {Cai, Tianqin and Zhang, Zhao and Yang, Ping},
booktitle = {Dl.Acm.Org},
isbn = {9781450379571},
keywords = {Model-based GUI testing,automatic testing,dynamic DAG exploration,multi-agent collaboration,traversal algorithm},
pages = {93--96},
title = {{Fastbot: A Multi-Agent Model-Based Test Generation System Beijing Bytedance Network Technology Co., Ltd.}},
url = {https://doi.org/10.1145/3387903.3389308},
year = {2020}
}
@inproceedings{Morgado2015,
abstract = {It is increasingly important to assess and ensure the correct behaviour of mobile applications as their importance in everyday life keeps increasing. This paper presents an automatic testing approach combining reverse engineering with testing. The algorithm tries to identify existing User Interface (UI) patterns on a mobile application under test through a reverse engineering process and then tests them using generic test strategies called Test Patterns. The overall testing approach was implemented in the iMPAcT (Mobile PAttern Testing) tool and is illustrated in a case study performed over some mobile applications as a proof-of-concept.},
author = {Morgado, Ines Coimbra and Paiva, Ana C.R.},
booktitle = {Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2015},
doi = {10.1109/ASEW.2015.11},
isbn = {9781467397759},
keywords = {GUI Testing,Mobile Testing,Pattern-based Testing,Reverse Engineering},
pages = {42--49},
publisher = {IEEE},
title = {{Testing approach for mobile applications through reverse engineering of UI patterns}},
year = {2016}
}
@article{Yan,
abstract = {Automatic event sequence generation tools are widely used for testing GUI applications. With these tools, developers can easily test the target GUI applications with a large number of events and collect a group of crash-triggering sequences in a short time. However, some efficiency-oriented tools generate low-level events randomly based on coordinates of the screen instead of widgets, which leads to many ineffective events that have no contribution to the test. Besides, the randomly generated sequences may repeatedly operate on the same widget or jump to the same window, which increases the complexity of sequences and makes it difficult to extract key events that can lead to crashes. The sequence reduction technique can effectively help developers to understand the crashes and further improve the quality of code. In this paper, we propose a general model for the event sequence reduction problem on GUI applications. For better illustration, we take the random test generation tool Monkey as a concrete instance, which is widely used for testing Android applications, owing to its simplicity, effectiveness and good compatibility. To address the major drawbacks in original Monkey testing, in this paper, we attempt to enhance Monkey to support the sequence record-and-replay and propose a sequence reduction approach for Android apps, which helps the crash behavior comprehension and fault localization. By manually investigating the effectiveness of Monkey events, we find three types of ineffective events, including no-ops, single and combination of effect-free ones, and design nine reduction rules for them. To extract key events in one sequence for crash understanding, we analyze the state transition relation among events and propose a static GUI state hierarchy-tree-guided reduction approach. Additionally, we implement our approach in a tool CHARD to achieve event sequence reduction on real-world apps. We also design a semi-structured format to describe the actual behavior of events and improve the sequence comprehensibility. We collect 890 sequences from 74 applications as our benchmark, including 740 basic sequences, each of which contains 1,000 events, and 150 longer ones, each of which contains 10,000 events. CHARD can quickly identify 41.3% events as ineffective ones in the collected sequences. For sequences that can be stably replayed, over 94% of the reduced sequences keep the same functionalities as the original ones. By removing ineffective events, CHARD can be used as a pre-process part of the traditional delta-debugging process and make significant speed up. To evaluate the effectiveness of the key event extraction approach, we pick eight buggy applications and collect 40 crash-triggering event sequences generated by Monkey, the length of which varies from 19 to 2700. The results show that CHARD can successfully remove over 95.4% crash-irrelevant events in these crash-triggering sequences within around ten seconds, while the state-of-the-art delta-debugging tool removes 71.3% ones using over 27 hours, which indicates that CHARD can efficiently help the crash replay and sequence comprehension.},
author = {Yan, Jiwei and Zhou, Hao and Deng, Xi and Wang, Ping and Yan, Rongjie and Yan, Jun and Zhang, Jian},
doi = {10.1016/j.scico.2020.102522},
issn = {01676423},
journal = {Science of Computer Programming},
keywords = {Android,GUI testing,Monkey,Record-and-replay,Test reduction},
pages = {102522},
publisher = {Elsevier},
title = {{Efficient testing of GUI applications by event sequence reduction}},
volume = {201},
year = {2021}
}
@article{Cheng2017a,
abstract = {Graphic user interface (GUI) is an integral part of many software applications. However, GUI testing remains a challenging task. The main problem is to generate a set of high-quality test cases, i.e., sequences of user events to cover the often large input space. Since manually crafting event sequences is labor-intensive and automated testing tools often have poor performance, we propose a new GUI testing framework to efficiently generate progressively longer event sequences while avoiding redundant sequences. Our technique for identifying the redundancy among these sequences relies on statically checking a set of simple and syntactic-level conditions, whose reduction power matches and sometimes exceeds that of classic techniques based on partial order reduction. We have evaluated our method on 17 Java Swing applications. Our experimental results show the new technique, while being sound and systematic, can achieve more than 10X reduction in the number of test sequences compared to the state-of-the-art GUI testing tools.},
author = {Cheng, Lin and Yang, Zijiang and Wang, Chao},
doi = {10.1109/ASE.2017.8115696},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Yang, Wang - 2017 - Systematic reduction of GUI test sequences.pdf:pdf},
isbn = {9781538626849},
journal = {ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
keywords = {GUI testing,test generation,event-driven program,p},
pages = {849--860},
title = {{Systematic reduction of GUI test sequences}},
year = {2017}
}
@inproceedings{Ardito2018,
abstract = {Market demands for faster delivery and higher software quality are progressively becoming more stringent. A key hindrance for software companies to meet such demands is how to test the software due to to the intrinsic costs of development, maintenance and evolution of testware. Especially since testware should be defined, and aligned, with all layers of system under test (SUT), including all graphical user interface (GUI) abstraction levels. These levels can be tested with different generations of GUI-based test approaches, where 2 nd generation, or Layout-based, tests leverage GUI properties and 3 rd generation, or Visual, tests make use of image recognition. The two approaches provide different benefits and drawbacks and are seldom used together because of the aforementioned costs, despite growing academic evidence of the complementary benefits. In this work we propose the proof of concept of a novel two-step translation approach for Android GUI testing that we aim to implement, where a translator first creates a technology independent script with actions and elements of the GUI, and then translates it to a script with the syntax chosen by the user. The approach enables users to translate Layout-based to Visual scripts and vice versa, to gain the benefits (e.g. robustness, speed and ability to emulate the user) of both generations, whilst minimizing the drawbacks (e.g. development and maintenance costs). We outline our approach from a technical perspective, discuss some of the key challenges with the realization of our approach, evaluate the feasibility and the advantages provided by our approach on an open-source Android application, and discuss the potential industrial impact of this work.},
author = {Ardito, Luca and Coppola, Riccardo and Torchiano, Marco and Al{\'{e}}groth, Emil},
booktitle = {Companion Proceedings for the ISSTA/ECOOP 2018 Workshops},
doi = {10.1145/3236454.3236488},
isbn = {9781450359399},
pages = {46--53},
publisher = {ACM},
title = {{Towards automated translation between generations of GUI-based tests for mobile devices}},
year = {2018}
}
@inproceedings{Hu2014,
abstract = {Mobile apps bring unprecedented levels of convenience, yet they are often buggy, and their bugs offset the convenience the apps bring. A key reason for buggy apps is that they must handle a vast variety of system and user actions such as being randomly killed by the OS to save resources, but app developers, facing tough competitions, lack time to thoroughly test these actions. AppDoctor is a system for efficiently and effectively testing apps against many system and user actions, and helping developers diagnose the resultant bug reports. It quickly screens for potential bugs using approximate execution, which runs much faster than real execution and exposes bugs but may cause false positives. From the reports, AppDoctor automatically verifies most bugs and prunes most false positives, greatly saving manual inspection effort. It uses action slicing to further speed up bug diagnosis. We implement AppDoctor in Android. It operates as a cloud of physical devices or emulators to scale up testing. Evaluation on 53 out of 100 most popular apps in Google Play and 11 of the most popular open-source apps shows that, AppDoctor effectively detects 72 bugs-including two bugs in the Android framework that affect all apps-with quick checking sessions, speeds up testing by 13.3 times, and vastly reduces diagnosis effort. Copyright {\textcopyright} 2007 by the Association for Computing Machinery, Inc.},
author = {Hu, Gang and Yuan, Xinhao and Tang, Yang and Yang, Junfeng},
booktitle = {Proceedings of the 9th European Conference on Computer Systems, EuroSys 2014},
doi = {10.1145/2592798.2592813},
isbn = {1450327044},
pages = {18},
publisher = {ACM},
title = {{Efficiently, effectively detecting mobile app bugs with appDoctor}},
year = {2014}
}
@article{Morgado2018,
abstract = {This paper presents a tool (iMPAcT) that automates testing of mobile applications based on the presence of recurring behaviour, UI Patterns. It combines reverse engineering, pattern matching and testing. The reverse engineering process is responsible for crawling the application, i.e. analysing the state of the application and interacting with it by firing events. The pattern matching tries to identify the presence of UI patterns based on a catalogue of patterns. When a UI Pattern from the catalogue is detected, a test strategy is applied (testing). These test strategies are called UI Test Patterns. These three phases work in an iterative way: the patterns are identified and tested between firing of events, i.e. the process alternates between exploring the application and testing the UI Patterns. The process is dynamic and fully automatic not requiring any previous knowledge about the application under test. This paper presents the results of an experiment studying the reliability of the results obtained by iMPAcT. The experiment involved 25 applications found on Google Play Store and concludes that iMPAcT is successful in identifying failures in the tested patterns and that the degree of certainty of an identified failure being an actual failure is high.},
author = {Morgado, In{\^{e}}s Coimbra and Paiva, Ana C.R.},
doi = {10.1007/s11219-017-9387-1},
issn = {15731367},
journal = {Software Quality Journal},
keywords = {Android,Case study,Mobile testing,Reverse engineering,UI patterns},
number = {4},
pages = {1553--1570},
publisher = {Springer},
title = {{Mobile GUI testing}},
volume = {26},
year = {2018}
}
@inproceedings{Clapp2016a,
author = {Clapp, Lazaro and Bastani, Osbert and Anand, Saswat and Aiken, Alex},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clapp et al. - 2016 - Minimizing GUI event traces.pdf:pdf},
isbn = {1450342183},
pages = {422--434},
publisher = {ACM},
title = {{Minimizing GUI event traces}},
year = {2016}
}
@inproceedings{Anand2012a,
author = {Anand, Saswat and Naik, Mayur and Harrold, Mary Jean and Yang, Hongseok},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tracz et al. - 2012 - Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (FSE-20) S.pdf:pdf},
isbn = {145031614X},
pages = {59},
publisher = {ACM},
title = {{Automated concolic testing of smartphone apps}},
year = {2012}
}
@inproceedings{Zhang2017a,
author = {Zhang, Chucheng and Cheng, Haoliang and Tang, Enyi and Chen, Xin and Bu, Lei and Li, Xuandong},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
file = {:C\:/Users/Lab/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Sketch-guided GUI test generation for mobile applications.pdf:pdf},
isbn = {1538626845},
pages = {38--43},
publisher = {IEEE Press},
title = {{Sketch-guided GUI test generation for mobile applications}},
year = {2017}
}
